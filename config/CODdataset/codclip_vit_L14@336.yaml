DATA:
  train_root: dataset/TrainDataset/
  val_root: dataset/TestDataset/CAMO/
  dataset: CODtraindataset
  # dataset: refcoco
  # train_lmdb: datasets/lmdb/refcoco/train.lmdb
  # train_split: train
  # val_lmdb: datasets/lmdb/refcoco/val.lmdb
  # val_split: val
  # mask_root: datasets/masks/refcoco
TRAIN:
  # Base Arch
  clip_pretrain: pretrain/ViT-L-14-336px.pt
  input_size: 336
  word_len: 77
  word_dim: 768
  feats_layer_num: [7, 15, 23]
  vis_dim: 768
  fpn_in: [768, 768, 768]  # defined by Vit
  fpn_out: [384, 768, 1024]
  sync_bn: True
  # Decoder
  num_layers: 3
  num_head: 8
  dim_ffn: 2048
  dropout: 0.1
  intermediate: False
  # Training Setting
  workers: 32  # data loader workers
  workers_val: 16
  epochs: 50
  milestones: [35]
  start_epoch: 0
  batch_size: 32  # batch size for training
  batch_size_val: 8  # batch size for validation during training, memory and speed tradeoff
  base_lr: 1e-4
  lr_decay: 0.1
  lr_multi: 0.1
  weight_decay: 0.
  max_norm: 0.
  manual_seed: 0
  print_freq: 100
  # Resume & Save
  exp_name: COD_vit_L14@336
  output_folder: exp/CODtraindataset
  save_freq: 1
  weight:  # path to initial weight (default: none)
  resume:  # path to latest checkpoint (default: none)
  evaluate: True  # evaluate on validation set, extra gpu memory needed and small batch_size_val is recommend
Distributed:
  dist_url: tcp://localhost:3681
  dist_backend: 'nccl'
  multiprocessing_distributed: True
  world_size: 1
  rank: 0
TEST:
  test_split: val-test
  save_path: pretrain/
  # test_lmdb: datasets/lmdb/refcoco/val.lmdb
  visualize: False